{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14758056,"sourceType":"datasetVersion","datasetId":9432788},{"sourceId":14758142,"sourceType":"datasetVersion","datasetId":9432853},{"sourceId":14760652,"sourceType":"datasetVersion","datasetId":9434605},{"sourceId":14762056,"sourceType":"datasetVersion","datasetId":9435533}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\n\n# Check if path was added\nprint(\"Current sys.path:\")\nfor p in sys.path[:5]:\n    print(f\"  {p}\")\n\n# Add path if not present\ncode_path = '/kaggle/input/src-code'\nif code_path not in sys.path:\n    sys.path.insert(0, code_path)\n    print(f\"\\n‚úì Added {code_path} to sys.path\")\n\n# Verify directory exists and contents\nprint(f\"\\nChecking {code_path}:\")\nif os.path.exists(code_path):\n    contents = os.listdir(code_path)\n    print(f\"  Contents: {contents}\")\n    \n    # Check for src\n    src_path = os.path.join(code_path, 'src')\n    if os.path.isdir(src_path):\n        print(f\"  ‚úì src/ found with: {os.listdir(src_path)}\")\n    else:\n        print(\"  ‚úó src/ folder not found!\")\n        # Maybe src is at root level of dataset?\n        if 'features' in contents or '__init__.py' in contents:\n            print(\"  ‚Üí src contents appear to be at root level\")\n            print(\"  ‚Üí Try: sys.path.insert(0, '/kaggle/input/src-code/src')\")\nelse:\n    print(\"  ‚úó Path doesn't exist!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:05.131935Z","iopub.execute_input":"2026-02-07T02:39:05.132572Z","iopub.status.idle":"2026-02-07T02:39:05.140709Z","shell.execute_reply.started":"2026-02-07T02:39:05.132544Z","shell.execute_reply":"2026-02-07T02:39:05.140077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# KAGGLE SETUP - TRANSLITERATION MODEL\n# ============================================\n\n# Installing dependencies\n!pip install pyyaml python-levenshtein pyarabic -q\n\nimport sys\nimport os\n\n# Adding src package to path\nsys.path.insert(0, '/kaggle/input/src-code')\n\n# Setting config directory before importing src modules\nfrom src.utils import config\nconfig._global_config = config.Config(config_dir='/kaggle/input/configs/')\n\nimport numpy as np\nimport pandas as pd\n\n# Importing src modules\nfrom src.features.character_encoder import ArabicCharEncoder, EnglishCharEncoder, EncoderPair\nfrom src.features.word_splitter import ArabicWordSplitter, create_word_level_dataset, validate_alignment\nfrom src.models.seq2seq_model import (\n    build_model, \n    ExactMatchCallback, \n    ScheduledSamplingCallback)\n\n# Loading data\ndf = pd.read_csv('/kaggle/input/model-clean-dataset/clean_word_pairs.csv')\ndf.rename(columns = {\n    'arabic_word':'arabic_name',\n    'english_word': 'english_name'\n}, inplace = True)\nprint(f\"‚úì Loaded {len(df)} word pairs\")\nprint(f\"\\nSample:\")\ndf.head(10)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:05.167394Z","iopub.execute_input":"2026-02-07T02:39:05.167934Z","iopub.status.idle":"2026-02-07T02:39:08.219669Z","shell.execute_reply.started":"2026-02-07T02:39:05.167912Z","shell.execute_reply":"2026-02-07T02:39:08.218957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# Suppress TensorFlow warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n\nimport tensorflow as tf\n\nfrom src.features.character_encoder import ArabicCharEncoder, EnglishCharEncoder, EncoderPair\nfrom src.features.word_splitter import ArabicWordSplitter, create_word_level_dataset, validate_alignment\nfrom src.models.seq2seq_model import (\n    build_model, \n    ExactMatchCallback, \n    ScheduledSamplingCallback\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:08.221432Z","iopub.execute_input":"2026-02-07T02:39:08.222006Z","iopub.status.idle":"2026-02-07T02:39:08.227509Z","shell.execute_reply.started":"2026-02-07T02:39:08.221972Z","shell.execute_reply":"2026-02-07T02:39:08.226951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 70/15/15 split\ntrain_val, test_df = train_test_split(df, test_size=0.15, random_state=42)\ntrain_df, val_df = train_test_split(train_val, test_size=0.15/0.85, random_state=42)\n\n# Save to working directory\ntrain_df.to_csv('/kaggle/working/train.csv', index=False)\nval_df.to_csv('/kaggle/working/val.csv', index=False)\ntest_df.to_csv('/kaggle/working/test.csv', index=False)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:08.228296Z","iopub.execute_input":"2026-02-07T02:39:08.228515Z","iopub.status.idle":"2026-02-07T02:39:08.251638Z","shell.execute_reply.started":"2026-02-07T02:39:08.228481Z","shell.execute_reply":"2026-02-07T02:39:08.250857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_word_level_data(train_path: str, val_path: str, test_path: str):\n    \"\"\"\n    Convert name-level data to word-level data.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PREPARING WORD-LEVEL DATA\")\n    print(\"=\"*60)\n\n    # Loading original data\n    train_df = pd.read_csv(train_path)\n    val_df = pd.read_csv(val_path)\n    test_df = pd.read_csv(test_path)\n\n    print(f\"Original data sizes:\")\n    print(f\"  Train: {len(train_df)} names\")\n    print(f\"  Val:   {len(val_df)} names\")\n    print(f\"  Test:  {len(test_df)} names\")\n\n    # Processing each dataset\n    train_words = create_word_level_dataset(train_df, 'arabic_name', 'english_name')\n    val_words = create_word_level_dataset(val_df, 'arabic_name', 'english_name')\n    test_words = create_word_level_dataset(test_df, 'arabic_name', 'english_name')\n\n    print(f\"\\nWord-level data sizes:\")\n    print(f\"  Train: {len(train_words)} word pairs\")\n    print(f\"  Val:   {len(val_words)} word pairs\")\n    print(f\"  Test:  {len(test_words)} word pairs\")\n\n    # Validating alignment on sample\n    print(\"\\n--- Sample Alignments (Validation) ---\")\n    validate_alignment(val_words, sample_size=10)\n\n    # Remove duplicates to get unique word pairs\n    train_unique = train_words.drop_duplicates(subset=['arabic_word', 'english_word'])\n    val_unique = val_words.drop_duplicates(subset=['arabic_word', 'english_word'])\n    test_unique = test_words.drop_duplicates(subset=['arabic_word', 'english_word'])\n\n    print(f\"\\nUnique word pairs:\")\n    print(f\"  Train: {len(train_unique)} unique\")\n    print(f\"  Val:   {len(val_unique)} unique\")\n    print(f\"  Test:  {len(test_unique)} unique\")\n\n    return train_unique, val_unique, test_unique\n\n\ndef build_vocabularies(train_words: pd.DataFrame):\n    \"\"\"Build character vocabularies from training data.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"BUILDING VOCABULARIES\")\n    print(\"=\"*60)\n    \n    # Collect all unique characters from training data\n    arabic_texts = train_words['arabic_word'].tolist()\n    english_texts = train_words['english_word'].tolist()\n    \n    # Get unique characters\n    arabic_chars = set()\n    for text in arabic_texts:\n        arabic_chars.update(text)\n    \n    english_chars = set()\n    for text in english_texts:\n        english_chars.update(text)\n    \n    # Build Arabic vocabulary\n    arabic_encoder = ArabicCharEncoder()\n    arabic_encoder.build_vocab(list(arabic_chars))\n    \n    # Build English vocabulary  \n    english_encoder = EnglishCharEncoder()\n    english_encoder.build_vocab(list(english_chars))\n    \n    encoder_pair = EncoderPair(arabic_encoder, english_encoder)\n    \n    print(f\"Arabic vocabulary: {arabic_encoder.vocab_size} characters\")\n    print(f\"English vocabulary: {english_encoder.vocab_size} characters\")\n    \n    return encoder_pair","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:08.253570Z","iopub.execute_input":"2026-02-07T02:39:08.254102Z","iopub.status.idle":"2026-02-07T02:39:08.263895Z","shell.execute_reply.started":"2026-02-07T02:39:08.254074Z","shell.execute_reply":"2026-02-07T02:39:08.263291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_sequences(encoder, texts, max_length, add_start=False, add_end=False):\n    \"\"\"Encode a batch of texts to padded sequences.\"\"\"\n    sequences = []\n    for text in texts:\n        indices = encoder.encode(text, add_start=add_start, add_end=add_end)\n        padded = encoder.pad_sequence(indices, max_length)\n        sequences.append(padded)\n    return np.array(sequences, dtype='int32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:08.264815Z","iopub.execute_input":"2026-02-07T02:39:08.265093Z","iopub.status.idle":"2026-02-07T02:39:08.283320Z","shell.execute_reply.started":"2026-02-07T02:39:08.265064Z","shell.execute_reply":"2026-02-07T02:39:08.282829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_training_data(words_df: pd.DataFrame, encoder_pair: EncoderPair, \n                          max_input_len: int = 20, max_output_len: int = 25):\n    \"\"\"Prepare encoded sequences for training.\"\"\"\n    \n    arabic_encoder = encoder_pair.arabic_encoder\n    english_encoder = encoder_pair.english_encoder\n    \n    arabic_texts = words_df['arabic_word'].tolist()\n    english_texts = words_df['english_word'].tolist()\n    \n    # Encode Arabic (input)\n    encoder_inputs = encode_sequences(arabic_encoder, arabic_texts, max_input_len)\n    \n    # Encode English with START token (decoder input)\n    decoder_inputs = encode_sequences(english_encoder, english_texts, max_output_len, \n                                       add_start=True, add_end=False)\n    \n    # Encode English with END token (decoder target)\n    decoder_targets = encode_sequences(english_encoder, english_texts, max_output_len,\n                                        add_start=False, add_end=True)\n    \n    return encoder_inputs, decoder_inputs, decoder_targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:08.284179Z","iopub.execute_input":"2026-02-07T02:39:08.284409Z","iopub.status.idle":"2026-02-07T02:39:08.298646Z","shell.execute_reply.started":"2026-02-07T02:39:08.284390Z","shell.execute_reply":"2026-02-07T02:39:08.298142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"Main training function.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"WORD-LEVEL TRANSLITERATION MODEL TRAINING\")\n    print(\"=\"*60)\n    \n    # Paths\n    data_dir = Path(\"/kaggle/working/\")\n    model_dir = Path(\"/kaggle/working/\")\n    model_dir.mkdir(exist_ok=True)\n    \n    # 1. Prepare word-level data\n    train_words, val_words, test_words = prepare_word_level_data(\n        data_dir / \"train.csv\",\n        data_dir / \"val.csv\", \n        data_dir / \"test.csv\"\n    )\n    \n    # 2. Build vocabularies\n    encoder_pair = build_vocabularies(train_words)\n    \n    # 3. Prepare training data\n    max_input_len = 20  # Max Arabic word length\n    max_output_len = 25  # Max English word length\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"PREPARING TRAINING SEQUENCES\")\n    print(\"=\"*60)\n    \n    train_enc, train_dec, train_tgt = prepare_training_data(\n        train_words, encoder_pair, max_input_len, max_output_len\n    )\n    val_enc, val_dec, val_tgt = prepare_training_data(\n        val_words, encoder_pair, max_input_len, max_output_len\n    )\n    \n    print(f\"Training shapes: input={train_enc.shape}, dec_input={train_dec.shape}, target={train_tgt.shape}\")\n    print(f\"Validation shapes: input={val_enc.shape}, dec_input={val_dec.shape}, target={val_tgt.shape}\")\n    \n    # 4. Build model\n    print(\"\\n\" + \"=\"*60)\n    print(\"BUILDING MODEL\")\n    print(\"=\"*60)\n    \n    model_config = {\n        'embedding_dim': 128,\n        'hidden_units': 256,\n        'attention_units': 64,\n        'dropout': 0.2,\n        'training': {\n            'learning_rate': 0.001\n        }\n    }\n    \n    model = build_model(\n        arabic_vocab_size=encoder_pair.arabic_encoder.vocab_size,\n        english_vocab_size=encoder_pair.english_encoder.vocab_size,\n        config=model_config\n    )\n    \n    # Build model with sample input\n    sample_enc = np.zeros((1, max_input_len), dtype='int32')\n    sample_dec = np.zeros((1, max_output_len), dtype='int32')\n    _ = model([sample_enc, sample_dec])\n    \n    print(f\"Model parameters: {model.count_params():,}\")\n    \n    # 5. Setup callbacks\n    print(\"\\n\" + \"=\"*60)\n    print(\"SETTING UP TRAINING\")\n    print(\"=\"*60)\n    \n    # Create validation dataframe for ExactMatchCallback\n    # The callback expects 'arabic_normalized' and 'english_cleaned' columns\n    val_df_for_callback = val_words.rename(columns={\n        'arabic_word': 'arabic_normalized',\n        'english_word': 'english_cleaned'\n    })\n    \n    callbacks = [\n        # Early stopping\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        # Learning rate reduction\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        # Exact match monitoring (smaller sample for word-level)\n        ExactMatchCallback(\n            val_df=val_df_for_callback,\n            encoder_pair=encoder_pair,\n            sample_size=50,  # Use more samples for word-level (faster)\n            max_length=max_output_len\n        ),\n        # Scheduled sampling to bridge teacher forcing gap\n        ScheduledSamplingCallback(\n            initial_prob=0.0,\n            final_prob=0.4,\n            warmup_epochs=10\n        )\n    ]\n\n# 6. Train\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING\")\n    print(\"=\"*60)\n    \n    batch_size = 64\n    epochs = 80\n    \n    history = model.fit(\n        [train_enc, train_dec],\n        train_tgt,\n        validation_data=([val_enc, val_dec], val_tgt),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    # ---------------------------------------------------------\n    # STEP 12: NEAR MISS ANALYSIS (Paste in a new cell)\n    # ---------------------------------------------------------\n        \n    print(\"\\nüîç NEAR MISS ANALYSIS (Comparing Greedy vs Beam Search)\")\n    print(\"=\"*60)\n    \n    # Pick 10 random samples from the validation set\n    indices = np.random.choice(len(val_enc), 10, replace=False)\n    \n    for idx in indices:\n        # 1. Get Inputs\n        input_seq = val_enc[idx:idx+1]\n        true_target = val_tgt[idx]\n        \n        # 2. Decode Truth (Remove padding/start/end)\n        true_text = encoder_pair.english_encoder.decode(\n            [i for i in true_target if i not in [0, 1, 2]]\n        )\n        \n        # 3. Get Greedy Prediction (Fast, single best guess)\n        greedy_pred, _ = model.predict_sequence(\n            input_seq, \n            encoder_pair.english_encoder, \n            max_length=25\n        )\n        \n        # 4. Get Beam Search Prediction (Slower, explores top-3 paths)\n        # Note: Ensure you are using the FIXED code I gave you previously\n        beam_pred, _ = model.predict_sequence_beam(\n            input_seq, \n            encoder_pair.english_encoder, \n            beam_width=3,  # Try increasing to 5 if results are still off\n            max_length=25\n        )\n        \n        # 5. Compare\n        match_icon = \"‚úÖ\" if beam_pred.strip() == true_text.strip() else \"‚ùå\"\n        \n        print(f\"Arabic: {encoder_pair.arabic_encoder.decode([i for i in input_seq[0] if i not in [0,1,2]])}\")\n        print(f\"True:   {true_text}\")\n        print(f\"Greedy: {greedy_pred}\")\n        print(f\"Beam:   {beam_pred}  {match_icon}\")\n        print(\"-\" * 30)\n    \n    # 7. Save model and encoders\n    print(\"\\n\" + \"=\"*60)\n    print(\"SAVING MODEL\")\n    print(\"=\"*60)\n    \n    model.save_weights(str(model_dir / \"word_model.weights.h5\"))\n    encoder_pair.save(str(model_dir / \"word_encoders\"))\n    \n    # Save model config\n    import json\n    config_path = model_dir / \"word_model_config.json\"\n    full_config = {\n        **model_config,\n        'arabic_vocab_size': encoder_pair.arabic_encoder.vocab_size,\n        'english_vocab_size': encoder_pair.english_encoder.vocab_size,\n        'max_input_len': max_input_len,\n        'max_output_len': max_output_len\n    }\n    with open(config_path, 'w') as f:\n        json.dump(full_config, f, indent=2)\n    \n    print(f\"‚úì Model weights saved to: {model_dir / 'word_model.weights.h5'}\")\n    print(f\"‚úì Encoders saved to: {model_dir / 'word_encoders'}\")\n    print(f\"‚úì Config saved to: {config_path}\")\n    \n    # 8. Print training summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETE\")\n    print(\"=\"*60)\n    \n    final_loss = history.history['loss'][-1]\n    final_val_loss = history.history['val_loss'][-1]\n    final_acc = history.history.get('accuracy', [0])[-1]\n    \n    print(f\"Final training loss: {final_loss:.4f}\")\n    print(f\"Final validation loss: {final_val_loss:.4f}\")\n    print(f\"Final training accuracy: {final_acc*100:.1f}%\")\n    \n    print(\"\\nTo evaluate: python evaluate_word_model.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T02:39:08.299490Z","iopub.execute_input":"2026-02-07T02:39:08.299727Z","iopub.status.idle":"2026-02-07T02:39:08.318537Z","shell.execute_reply.started":"2026-02-07T02:39:08.299707Z","shell.execute_reply":"2026-02-07T02:39:08.318015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T08:49:32.319919Z","iopub.execute_input":"2026-02-07T08:49:32.320144Z","iopub.status.idle":"2026-02-07T08:49:32.327394Z","shell.execute_reply.started":"2026-02-07T08:49:32.320104Z","shell.execute_reply":"2026-02-07T08:49:32.326564Z"}},"outputs":[],"execution_count":null}]}